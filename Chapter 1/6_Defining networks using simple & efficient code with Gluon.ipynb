{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Defining networks using simple & efficient code with Gluon.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "amf55lkoljYB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import gluon**"
      ]
    },
    {
      "metadata": {
        "id": "2QiAPp8iibl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import mxnet as mx\n",
        "from mxnet import gluon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBjTEQwXlmWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating some dummy data. For this, we need the data to be in MXNet's NDArray or Symbol:**"
      ]
    },
    {
      "metadata": {
        "id": "PwpeH-ZsjkS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x_input = mx.nd.empty((1, 5), mx.cpu())\n",
        "x_input[:] = np.array([[1, 2, 3, 4, 5]], np.float32)\n",
        "\n",
        "y_input = mx.nd.empty((1, 5), mx.cpu())\n",
        "y_input[:] = np.array([[10, 15, 20, 22.5, 25]], np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clcXY-hzlu15",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**With Gluon, it's really straightforward to build a neural network by stacking layers:**"
      ]
    },
    {
      "metadata": {
        "id": "AXNMmp3hj7aX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = gluon.nn.Sequential()\n",
        "\n",
        "with net.name_scope():\n",
        "  net.add(gluon.nn.Dense(16, activation = \"relu\"))\n",
        "  net.add(gluon.nn.Dense(len(y_input)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DfV_8R-zl6Gl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Next, we initialize the parameters and we store these on our CPU as follows:**"
      ]
    },
    {
      "metadata": {
        "id": "YGVEniWNkMC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net.collect_params().initialize(mx.init.Normal(), ctx = mx.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-Ub1KAbmBee",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**With the following code, we set the loss function and the optimizer:**"
      ]
    },
    {
      "metadata": {
        "id": "w-LTiTnbkWzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "\n",
        "trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                        {'learning_rate': .1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5ECrcwemI0a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Finally, we train the model**"
      ]
    },
    {
      "metadata": {
        "id": "4npvofEcku0T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  for i in range(len(x_input)):\n",
        "    input = x_input[i]\n",
        "    target = y_input[i]\n",
        "    with mx.autograd.record():\n",
        "      output = net(input)\n",
        "      loss = softmax_cross_entropy(output, target)\n",
        "      loss.backward()\n",
        "    trainer.step(input.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}